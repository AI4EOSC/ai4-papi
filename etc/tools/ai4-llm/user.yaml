---
# User customizable configuration to make a deployment in Nomad.
# Additional non-customizable values (eg. ports) are hardcoded in `job.nomad`.

# All conf parameters follow the same structure:
# varname:
#  name: name of the parameter to be displayed to end user (mandatory)
#  value: (default) value of the parameter (mandatory)
#  options: restricted set of values that the parameter can take (optional)
#  description: some comments on the parameter to be displayed to the end user (optional)

general:

  title:
    name: Deployment title
    value: ''
    description: Provide short title for this deployment (less than 45 characters). Useful when you have lots of different active deployments.

  desc:
    name: Deployment description
    value: ''
    description: Provide some additional extended information about this deployment.

  type:
    name: Deployment type
    value: 'both'
    description: Sub-components to deploy.
    options: ['both', 'vllm', 'open-webui']

  docker_image:
    name: Docker image
    value: 'vllm/vllm-openai'
    description: Docker image to be used. For example `deephdc/deep-oc-image-classification-tf`.

  docker_tag:
    name: Docker tag
    value: 'latest'
    description: Docker tag to use. Tags are module dependent. You should choose the appropriate tag for your selected hardware (eg. use a `gpu`-like tag if you plan to run on GPUs).
    options: ['latest']


vllm:

  gpu_memory_utilization:
    name: GPU memory utilization
    value:
    range: [0, 1]
    description: Fraction of GPU memory to be used.

  max_model_length:
    name: Maximum model length
    value:
    description: Maximum length of the model.

  tensor_parallel_size:
    name: Tensor parrallel size
    value:
    description: Number of tensor parrallel size.

  huggingface_token:
    name: Huggingface token
    value: ''
    description: Huggingface token to use.

  vllm_model:
    name: VLLM model
    value: 'Qwen/Qwen2.5-1.5B-Instruct'
    description: VLLM model to use.
